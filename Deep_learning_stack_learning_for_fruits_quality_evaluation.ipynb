{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo0cHlLE6q2AR+4KX41gwe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhnidh/Using-stacking-learning-approach-in-Fruit-assessment-with-Deep-learning/blob/main/Deep_learning_stack_learning_for_fruits_quality_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SET 2"
      ],
      "metadata": {
        "id": "Kn4dlqh9h5KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2 as cv\n",
        "from tqdm import tqdm\n",
        "\n",
        "#loading images from path\n",
        "DATADIR = 'C:/Users/divya/Desktop/set 2/archive2/dataset/train'\n",
        "CATEGORIES = [\"freshapples\",\"freshbanana\",\"freshoranges\",\"rottenapples\",\"rottenbanana\",\"rottenoranges\"]\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(DATADIR, category)\n",
        "    for img in os.listdir(path):\n",
        "        img_array = cv.imread(os.path.join(path,img))\n",
        "        #,cv2.IMREAD_GRAYSCALE\n",
        "        plt.imshow(img_array)\n",
        "        plt.show()\n",
        "        break\n",
        "    break\n",
        "#print (room_types)  #what kinds of rooms are in this dataset\n",
        "\n",
        "print(\"Types of fruits found: \", len(CATEGORIES))"
      ],
      "metadata": {
        "id": "hu6bDmbwh6fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Types of fruits found:  6\n",
        "IMG_SIZE = 100\n",
        "batch_size = 100\n",
        "img_height = 100\n",
        "img_width = 100"
      ],
      "metadata": {
        "id": "dHr2trpxiMWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#building our training data\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  # do fruits\n",
        "\n",
        "        path = os.path.join(DATADIR,category)  # create path to fruits\n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 ,1,2,3,4,5,6) different number donate different category of fruit\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):  # iterate over each image per fruits\n",
        "            try:\n",
        "                img_array = cv.imread(os.path.join(path,img) ,cv.IMREAD_GRAYSCALE)  # convert to array\n",
        "                new_array = cv.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                pass\n",
        "\n",
        "create_training_data()\n",
        "\n",
        "image_count = len(training_data)"
      ],
      "metadata": {
        "id": "Ot3i9ZRqiYEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a dataframe\n",
        "fruits_df = pd.DataFrame(data=training_data, columns=['fruits type', 'image'])\n",
        "print(len(fruits_df))\n"
      ],
      "metadata": {
        "id": "QsD95g-MiZoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build a dataframe\n",
        "fruits_df = pd.DataFrame(data=training_data, columns=['fruits type', 'image'])\n"
      ],
      "metadata": {
        "id": "ieqEymeOirPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Assigning labels and features\n",
        "X =[]\n",
        "y =[]\n",
        "for features, label in training_data:\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
      ],
      "metadata": {
        "id": "muUYWTxgir45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the image array to a numpy type\n",
        "X = np.array(X)\n",
        "X.shape\n",
        "X = np.array(X)\n",
        "y=np.array(y)"
      ],
      "metadata": {
        "id": "kmM05QfEiw-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating own model\n",
        "#Normalising X and converting labels to categorical data\n",
        "from keras.utils import np_utils\n",
        "X = X.astype('float32')\n",
        "X /= 255\n",
        "Y = np_utils.to_categorical(y,6)\n",
        "#print(Y[100])\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "-dJ9_Wphi3_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, Y = shuffle(X, Y, random_state=1)\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.3, random_state=415)\n",
        "\n",
        "#inpect the shape of the training and testing.\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ],
      "metadata": {
        "id": "PVHIQeUki4Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n"
      ],
      "metadata": {
        "id": "C9z92oFhjM5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x is input, y=F(x)\n",
        "# identity block simply means input should be equal to output.\n",
        "#  y = x + F(x)   the layers in a traditional network are learning the true output H(x)\n",
        "# F(x) = y - x   the layers in a residual network are learning the residual F(x)\n",
        "# Hence, the name: Residual Block.\n",
        "def identity_block(A, f, filters, stage, block):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A -- input of shape (m, height, width, channel)\n",
        "    f -- shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "\n",
        "    Returns:\n",
        "    A -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Saving the input value.we need this later to add to the output.\n",
        "    A_shortcut = A\n",
        "     # First component of main path\n",
        "    A = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a')(A)\n",
        "    A = BatchNormalization(axis = 3, name = bn_name_base + '2a')(A)\n",
        "    A = Activation('relu')(A)\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    A = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b')(A)\n",
        "    A = BatchNormalization(axis = 3, name = bn_name_base + '2b')(A)\n",
        "    A = Activation('relu')(A)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    A = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c')(A)\n",
        "    A = BatchNormalization(axis = 3, name = bn_name_base + '2c')(A)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    A = Add()([A, A_shortcut])\n",
        "    A = Activation('relu')(A)\n",
        "\n",
        "    return A\n"
      ],
      "metadata": {
        "id": "5NXLbY6ZjWYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convolutional_block(A, f, filters, stage, block, s = 2):\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    # Save the input value\n",
        "    A_shortcut = A\n",
        "    # First layer\n",
        "    A = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a')(A) # 1,1 is filter size\n",
        "    A = BatchNormalization(axis = 3, name = bn_name_base + '2a')(A)  # normalization on channels\n",
        "    A = Activation('relu')(A)\n",
        "    # Second layer  (f,f)=3*3 filter by default\n",
        "    A = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b')(A)\n",
        "    A = BatchNormalization(axis = 3, name = bn_name_base + '2b')(A)\n",
        "    A = Activation('relu')(A)\n",
        "    # Third layer\n",
        "    A = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c')(A)\n",
        "    A = BatchNormalization(axis = 3, name = bn_name_base + '2c')(A)\n",
        "     ##### SHORTCUT PATH ####\n",
        "    A_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1')(A_shortcut)\n",
        "    A_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(A_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value here, and pass it through a RELU activation\n",
        "    A = Add()([A, A_shortcut])\n",
        "    A = Activation('relu')(A)\n",
        "\n",
        "    return A\n"
      ],
      "metadata": {
        "id": "DMcV1sgOjWua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating Resnet 50\n",
        "#Each ResNet block is either 2 layer deep\n",
        "def ResNet50(input_shape=(100,100,1), classes=7):\n",
        "    \"\"\"\n",
        "    Implementation of the ResNet50 architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAAPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    \"\"\"\n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    A_input = Input(input_shape)\n",
        "    # Zero-Padding\n",
        "    A = ZeroPadding2D((3, 3))(A_input) #3,3 padding\n",
        "    # Stage 1\n",
        "    A = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(A) #64 filters of 7*7\n",
        "    A = BatchNormalization(axis=3, name='bn_conv1')(A) #batch norm applied on channels\n",
        "    A = Activation('relu')(A)\n",
        "    A = MaxPooling2D((3, 3), strides=(2, 2))(A) #window size is 3*3\n",
        "    # Stage 2\n",
        "    A = convolutional_block(A, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
        "    # convolutional_block is a function defined above. Convolutional_block have 3 layers.\n",
        "    #filters=[64, 64, 256] first 64 is for 1st layer and 2nd 64 is for 2nd layer and 256 is for 3rd layer of convultional block\n",
        "    # below are the conv layers from convolutional_block function\n",
        "    #A = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a')(A)\n",
        "    #A = Conv2D(F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b')(A)\n",
        "    #A = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '2a')(A)\n",
        "    A = identity_block(A, 3, [64, 64, 256], stage=2, block='b')\n",
        "    #A = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a')(A)\n",
        "    #A = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b')(A)\n",
        "    #A = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c')(A)\n",
        "    A = identity_block(A, 3, [64, 64, 256], stage=2, block='c')\n",
        "    #A = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a')(A)\n",
        "    #A = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b')(A)\n",
        "    #A = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c')(A)\n",
        "    ### START CODE HERE ###\n",
        "    # Stage 3\n",
        "    A = convolutional_block(A, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
        "    A = identity_block(A, 3, [128, 128, 512], stage=3, block='b')\n",
        "    A = identity_block(A, 3, [128, 128, 512], stage=3, block='c')\n",
        "    A = identity_block(A, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4\n",
        "    A = convolutional_block(A, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
        "    A = identity_block(A, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    A = identity_block(A, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    A = identity_block(A, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    A = identity_block(A, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    A = identity_block(A, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5\n",
        "    A = convolutional_block(A, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
        "    A = identity_block(A, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    A = identity_block(A, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL\n",
        "    A = AveragePooling2D((2,2), name=\"avg_pool\")(A)\n",
        "     ### END CODE HERE ###\n",
        "    # output layer\n",
        "    A = Flatten()(A)\n",
        "    A = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(A)\n",
        "    # Create model\n",
        "    model1 = Model(inputs = A_input, outputs = A, name='ResNet50')\n",
        "\n",
        "    return model1\n"
      ],
      "metadata": {
        "id": "45Lgytxxjidz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = ResNet50(input_shape = (100,100,1), classes = 6)\n"
      ],
      "metadata": {
        "id": "rt-Z9hkljqcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "p3sddOpBjqo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()\n"
      ],
      "metadata": {
        "id": "UPuE-yk0jxUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model1.fit(train_x, train_y,batch_size=batch_size,epochs=12,validation_data=(test_x, test_y),shuffle=True,callbacks=[tf.keras.callbacks.CSVLogger('hisr.csv')])"
      ],
      "metadata": {
        "id": "140c_W8Rjxi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.save('saved_models/model1.hdf5')\n"
      ],
      "metadata": {
        "id": "uMxWvVzIkcix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the Model\n",
        "history = pd.read_csv('hisr.csv')\n",
        "history.head()\n",
        "plt.figure(figsize=(19,6))\n",
        "plt.subplot(131)\n",
        "plt.plot(history.epoch, history.loss, label=\"loss\")\n",
        "plt.plot(history.epoch, history.val_accuracy, label=\"val_accuracy\")\n",
        "plt.xlabel('loss')\n",
        "plt.ylabel('val_accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "XLCgwx8dkcu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#observing its classification report and confusion matrix\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "#predict\n",
        "y_pred=model1.predict(test_x)\n",
        "y_pred=np.argmax(y_pred,axis=1)\n",
        "test_y_arg=np.argmax(test_y,axis=1)\n",
        "Y_pred = np.argmax(model1.predict(test_x),axis=1)\n"
      ],
      "metadata": {
        "id": "RXgQ4sF7kloU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(96, (3,3), padding='same', activation=tf.nn.relu,input_shape=(100, 100, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=3),\n",
        "    tf.keras.layers.Conv2D(256, (3,3), padding='same', activation=tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=3),\n",
        "    tf.keras.layers.Conv2D(384, (3,3), padding='same', activation=tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=3),\n",
        "    tf.keras.layers.Conv2D(384, (3,3), padding='same', activation=tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=3),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(6,  activation=tf.nn.softmax)\n",
        "])\n"
      ],
      "metadata": {
        "id": "EJBTMk4Mkl3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "id": "-Sd3v7hvkxJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model2.fit(train_x, train_y, batch_size = 32,verbose=1, epochs = 12,validation_split = 0.3, callbacks=[tf.keras.callbacks.CSVLogger('his.csv')])"
      ],
      "metadata": {
        "id": "0iw2igJ6kxUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.save('saved_model/model2.hdf5')\n"
      ],
      "metadata": {
        "id": "Yij2kNG5k8ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the Model\n",
        "history = pd.read_csv('his.csv')\n",
        "history.head()"
      ],
      "metadata": {
        "id": "mUsPdIrLk8xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(19,6))\n",
        "plt.subplot(131)\n",
        "plt.plot(history.epoch, history.loss, label=\"loss\")\n",
        "plt.plot(history.epoch, history.val_accuracy, label=\"val_accuracy\")\n",
        "plt.xlabel('loss')\n",
        "plt.ylabel('val_accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "nmnUNrTklGOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "rounded_predictions = model2.predict(test_x, batch_size=32, verbose=0)\n",
        "rounded_predictions[1]"
      ],
      "metadata": {
        "id": "Fun3RuHUlGZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy and Score of model\n",
        "score = model2.evaluate(test_x, test_y, verbose = 1 )\n",
        "print(\"Test Score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1]*100,'%')"
      ],
      "metadata": {
        "id": "FehbMdWslS0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy and Score of model\n",
        "#score = model.evaluate(X_test, y_test, verbose = 1 )\n",
        "#print(\"Test Score: \", score[0])\n",
        "#print(\"Test accuracy: \", score[1]*100,'%')\n",
        "#observing its classification report and confusion matrix\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "#predict\n",
        "y_pred=model2.predict(test_x)\n",
        "y_pred=np.argmax(y_pred,axis=1)\n",
        "test_y_arg=np.argmax(test_y,axis=1)\n"
      ],
      "metadata": {
        "id": "F1FjERttlTEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Conv2D layer with 32 filters and a kernel size of (3, 3) followed by a ReLU activation function. This layer takes in the input shape of (100, 100, 3). A MaxPooling2D layer that performs max pooling with pool size of (2, 2) to reduce the spatial dimensions. A Dropout layer with a rate of 0.25 to prevent overfitting. Another Conv2D layer with 64 filters and a kernel size of (3, 3), followed by a ReLU activation function. Another MaxPooling2D layer with pool size of (2, 2) and a Dropout layer with a rate of 0.25. Another Conv2D layer with 128 filters and a kernel size of (3, 3), followed by a ReLU activation function. Another MaxPooling2D layer with pool size of (2, 2) and a Dropout layer with a rate of 0.25. A Flatten layer to flatten the output from the convolutional layers into a 1D vector. Two dense (Dense) layers with 512 and 256 neurons respectively, using ReLU activation functions and a Dropout layer with a rate of 0.5. A final dense (Dense) layer with 10 neurons and a Softmax activation function. Finally, we compile the model using the Adam optimizer, categorical cross-entropy loss function, and accuracy metric.\n",
        "\n",
        "With this model architecture, you can experiment with different hyperparameters and train the model on your dataset to achieve higher accuracy.\n"
      ],
      "metadata": {
        "id": "ZeGPkE3-lr0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the model3 architecture\n",
        "model3 = Sequential()\n",
        "model3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100, 100, 1)))\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(Dropout(0.25))\n",
        "model3.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(Dropout(0.25))\n",
        "model3.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(Dropout(0.25))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(512, activation='relu'))\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(256, activation='relu'))\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model3\n",
        "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "pUqUu8oIlfq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history3=model3.fit(train_x, train_y,batch_size=batch_size,epochs=12,validation_data=(test_x, test_y),shuffle=True,callbacks=[tf.keras.callbacks.CSVLogger('hisr3.csv')])\n"
      ],
      "metadata": {
        "id": "VEZTin5_lf3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.save('saved_models/model3.hdf5')"
      ],
      "metadata": {
        "id": "EtnVYYT_l6va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # plot the graph for 3rd model\n",
        "#Evaluate the Model\n",
        "history = pd.read_csv('hisr3.csv')\n",
        "history.head()\n",
        "plt.figure(figsize=(19,6))\n",
        "plt.subplot(131)\n",
        "plt.plot(history.epoch, history.loss, label=\"loss\")\n",
        "plt.plot(history.epoch, history.val_accuracy, label=\"val_accuracy\")\n",
        "plt.xlabel('loss')\n",
        "plt.ylabel('val_accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "u9vAz31jmAWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "#predict\n",
        "y_pred=model3.predict(test_x)\n",
        "y_pred=np.argmax(y_pred,axis=1)\n",
        "\n",
        "#get confusion matrix\n",
        "test_y_arg=np.argmax(test_y,axis=1)\n",
        "#print('Confusion Matrix')\n",
        "#print(confusion_matrix(test_y_arg, y_pred))\n",
        "#print()\n",
        "#print(',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,')\n",
        "#print()\n",
        "#get classification report\n",
        "#print(classification_report(y_pred,test_y_arg))\n"
      ],
      "metadata": {
        "id": "_A9bnGOcmAfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "model1=load_model('saved_models/model1.hdf5')\n",
        "model2=load_model('saved_model/model2.hdf5')\n",
        "model3=load_model('saved_models/model3.hdf5')\n",
        "models=[model1,model2,model3]"
      ],
      "metadata": {
        "id": "S91ShANAmAic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=[model.predict(test_x) for model in models]\n",
        "preds=np.array(preds)\n",
        "summed=np.sum(preds,axis=0)\n",
        "ensemble_prediction = np.argmax(summed,axis=1)"
      ],
      "metadata": {
        "id": "CxtsNPJqmTJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction1=model1.predict(test_x)\n",
        "prediction2=model2.predict(test_x)\n",
        "prediction3=model3.predict(test_x)"
      ],
      "metadata": {
        "id": "4C7Z_0XnmTWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#argamx across classes\n",
        "esemble_prediction=np.argmax(summed,axis=1)\n",
        "esemble_prediction = np_utils.to_categorical(esemble_prediction,6)\n",
        "prediction1=np.array(prediction1)\n",
        "prediction2=np.array(prediction2)\n",
        "prediction3=np.array(prediction3)\n",
        "test_y_arg=np.argmax(test_y,axis=1)"
      ],
      "metadata": {
        "id": "buVnnHB3mahD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model1 confusion matrics\")\n",
        "print(confusion_matrix(np.argmax(test_y, axis=1), np.argmax(prediction1, axis=1)))\n",
        "#classification_report(np.argmax(test_y, axis=1), np.argmax(prediction1, axis=1))\n",
        "print(',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,')\n",
        "print(\"model2 confusion matrics\")\n",
        "print(confusion_matrix(np.argmax(test_y, axis=1), np.argmax(prediction2, axis=1)))\n",
        "print(',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,')\n",
        "print(\"model3 confusion matrics\")\n",
        "print(confusion_matrix(np.argmax(test_y, axis=1), np.argmax(prediction3, axis=1)))"
      ],
      "metadata": {
        "id": "kaWq1d-mmar0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"esemble_model confusion matrics\")\n",
        "print(confusion_matrix(np.argmax(test_y, axis=1), np.argmax(esemble_prediction, axis=1)))\n"
      ],
      "metadata": {
        "id": "QkOcCDqUmsnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(test_y, axis=1), np.argmax(prediction1, axis=1)))\n",
        "print(',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,')\n",
        "print(classification_report(np.argmax(test_y, axis=1), np.argmax(prediction2, axis=1)))\n",
        "print(',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,')\n",
        "print(classification_report(np.argmax(test_y, axis=1), np.argmax(prediction3, axis=1)))\n",
        "print(',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,')\n",
        "print(\"Esemble learning classification report\")\n",
        "print(classification_report(np.argmax(test_y, axis=1), np.argmax(esemble_prediction, axis=1)))\n"
      ],
      "metadata": {
        "id": "_aHJne8hmtaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting accuracy of first model\n",
        "y_pred=model1.predict(test_x)\n",
        "y_pred=np.argmax(y_pred, axis=1) y_test=np.argmax(test_y, axis=1) accuracy1=accuracy_score(y_test,prediction1)\n"
      ],
      "metadata": {
        "id": "txr1a1Whm9Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = model1.evaluate(test_x, test_y, verbose = 1 )\n",
        "print(\"Test Score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])\n",
        "print(',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,')\n",
        "score = model2.evaluate(test_x, test_y, verbose = 1 )\n",
        "print(\"Test Score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])\n",
        "print(',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,')\n",
        "score = model3.evaluate(test_x, test_y, verbose = 1 )\n",
        "print(\"Test Score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "metadata": {
        "id": "MqhihyammzJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_accuracy = accuracy_score(test_y,esemble_prediction)\n",
        "print(ensemble_accuracy)"
      ],
      "metadata": {
        "id": "CzRB2nvRnIix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now design algorithm for ensemble learning using meta data and above 2 model.\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Average\n",
        "model1 = load_model('saved_models/model1.hdf5')\n",
        "model_1 = Model(inputs=model1.inputs,\n",
        "                outputs=model1.outputs,\n",
        "                name='name_of_model_1')\n",
        "model2 = load_model('saved_model/model2.hdf5')\n",
        "model_2 = Model(inputs=model2.inputs,\n",
        "                outputs=model2.outputs,\n",
        "                name='name_of_model_2')\n",
        "model3 = load_model('saved_models/model3.hdf5')\n",
        "model_3 = Model(inputs=model3.inputs,\n",
        "                outputs=model3.outputs,\n",
        "                name='name_of_model_3')\n",
        "models = [model_1, model_2,model_3]\n",
        "model_input = Input(shape=(100, 100, 1))\n",
        "model_outputs = [model(model_input) for model in models]\n",
        "ensemble_output = Average()(model_outputs)\n",
        "ensemble_model = Model(inputs=model_input, outputs=ensemble_output, name='ensemble')\n"
      ],
      "metadata": {
        "id": "kI85jNuknO04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "#model1=load_model('saved_models/model.hdf5')\n",
        "#model2=load_model('saved_model/model2.hdf5')\n",
        "#model2=load_model('saved_models/model2.hdf5')\n",
        "#models=[model,model2,model3]\n",
        "\n",
        "preds= [model.predict(test_x) for model in models]\n",
        "preds = np.array(preds)\n",
        "summed =np.sum(preds,axis=0)"
      ],
      "metadata": {
        "id": "onkD4WuunPhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#esemble learning using weighted average of different model\n",
        "models=[model1,model2,model3]\n",
        "preds = [model.predict(test_x) for model in models]\n",
        "preds=np.array(preds)"
      ],
      "metadata": {
        "id": "a8dux65EnZhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction3=model3.predict(test_x)\n",
        "#argamx across classes\n",
        "esemble_prediction=np.argmax(summed,axis=1)\n",
        "esemble_prediction = np_utils.to_categorical(esemble_prediction,6)"
      ],
      "metadata": {
        "id": "_BjP8a9oneFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights=[0.2,0.4,0.4]\n",
        "#weighted_preds = np.tensordot(preds.transpose((1, 2, 0)), weights, axes=((0),(0)))\n",
        "weighted_preds = np.tensordot(preds,weights,axes=((0),(0))).shape\n",
        "weighted_ensemble_prediction = np.argmax(weighted_preds,axis=0)\n",
        "weighted_accuracy = accuracy_score(test_y,weighted_ensemble_prediction)\n",
        "print(weighted_accuracy)"
      ],
      "metadata": {
        "id": "3-XpXdOXnji9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack the predictions\n",
        "stacked_predictions = np.hstack((prediction1, prediction2, prediction3))\n",
        "\n",
        "# Define the meta-learner\n",
        "meta_learner = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(30,)),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the meta-learner\n",
        "meta_learner.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "meta_learner.fit(stacked_predictions, y_test, batch_size=64, epochs=5)\n",
        "\n",
        "# Generate predictions using the stacked ensemble\n",
        "stensemble_predictions = meta_learner.predict(stacked_predictions)\n",
        "\n",
        "# Evaluate the ensemble accuracy\n",
        "ensemble_accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(stensemble_predictions, axis=1))\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n"
      ],
      "metadata": {
        "id": "CDJ2l151ntZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "model1=load_model('saved_models/model1.hdf5')\n",
        "model2=load_model('saved_model/model2.hdf5')\n",
        "model3=load_model('saved_models/model3.hdf5')\n",
        "models=[model1,model2,model3]"
      ],
      "metadata": {
        "id": "kOeJQNZpnt_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2 as cv\n",
        "from tqdm import tqdm\n",
        "\n",
        "#loading images from path\n",
        "DATADIR = 'C:/Users/divya/Desktop/set 2/archive2/dataset/train'\n",
        "CATEGORIES = [\"freshapples\",\"freshbanana\",\"freshoranges\",\"rottenapples\",\"rottenbanana\",\"rottenoranges\"]\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(DATADIR, category)\n",
        "    for img in os.listdir(path):\n",
        "        img_array = cv.imread(os.path.join(path,img))\n",
        "        #,cv2.IMREAD_GRAYSCALE\n",
        "        plt.imshow(img_array)\n",
        "        plt.show()\n",
        "        break\n",
        "    break\n",
        "#print (room_types)  #what kinds of rooms are in this dataset\n",
        "\n",
        "print(\"Types of fruits found: \", len(CATEGORIES))\n"
      ],
      "metadata": {
        "id": "53EPGLjony04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Types of fruits found:  6\n",
        "IMG_SIZE = 100\n",
        "batch_size = 100\n",
        "img_height = 100\n",
        "img_width = 100\n",
        "#building our training data\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  # do fruits\n",
        "\n",
        "        path = os.path.join(DATADIR,category)  # create path to fruits\n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 ,1,2,3,4,5,6) different number donate different category of fruit\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):  # iterate over each image per fruits\n",
        "            try:\n",
        "                img_array = cv.imread(os.path.join(path,img) ,cv.IMREAD_GRAYSCALE)  # convert to array\n",
        "                new_array = cv.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                pass\n",
        "\n",
        "create_training_data()\n",
        "\n",
        "image_count = len(training_data)"
      ],
      "metadata": {
        "id": "jMTv0SleoC-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning labels and features\n",
        "X =[]\n",
        "y =[]\n",
        "for features, label in training_data:\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
      ],
      "metadata": {
        "id": "CHeYQrk-oKTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating own model\n",
        "#Normalising X and converting labels to categorical data\n",
        "from keras.utils import np_utils\n",
        "X = X.astype('float32')\n",
        "X /= 255\n",
        "Y = np_utils.to_categorical(y,6)\n",
        "#print(Y[100])\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "cXy1IZwjoOt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, Y = shuffle(X, Y, random_state=1)\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "gTbe2B80oUNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction1=model1.predict(test_x)\n",
        "prediction2=model2.predict(test_x)\n",
        "prediction3=model3.predict(test_x)\n"
      ],
      "metadata": {
        "id": "-Uvgx3IZoVBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the input for the meta-model by combining the predictions of the base models\n",
        "stacked_predictions = np.column_stack((prediction1, prediction2, prediction3))\n",
        "\n",
        "# Define the meta-model (Random Forest Classifier) and its hyperparameters for tuning\n",
        "meta_model = RandomForestClassifier()\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Perform hyperparameter tuning using GridSearchCV\n",
        "grid_search = GridSearchCV(meta_model, param_grid, cv=5)\n",
        "grid_search.fit(stacked_predictions, test_y)\n",
        "\n",
        "# Retrieve the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train the meta-model on the stacked predictions with the best hyperparameters\n",
        "meta_model = RandomForestClassifier(**best_params)\n",
        "meta_model.fit(stacked_predictions, test_y)\n",
        "\n",
        "# Make final predictions using the meta-model\n",
        "final_pred = meta_model.predict(stacked_predictions)\n",
        "\n",
        "# Evaluate the accuracy of the stacking ensemble\n",
        "accuracy = accuracy_score(test_y, final_pred)\n",
        "print(\"Accuracy of stacking ensemble:\", accuracy)\n"
      ],
      "metadata": {
        "id": "hIqTLZU8oXsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming you have the true labels in y_test and the final predictions in final_pred\n",
        "\n",
        "# Flatten the true labels and final predictions\n",
        "y_test_flat = np.argmax(test_y, axis=1)\n",
        "final_pred_flat = np.argmax(final_pred, axis=1)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test_flat, final_pred_flat)\n",
        "\n",
        "# Create a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test_flat, final_pred_flat)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "JL9TMHgPohmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming you have the true labels in y_test and the final predictions in final_pred\n",
        "\n",
        "# Define the category labels\n",
        "category_labels = [\"freshapples\", \"freshbanana\", \"freshoranges\", \"rottenapples\", \"rottenbanana\", \"rottenoranges\"]\n",
        "\n",
        "# Flatten the true labels and final predictions\n",
        "y_test_flat = np.argmax(test_y, axis=1)\n",
        "final_pred_flat = np.argmax(final_pred, axis=1)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test_flat, final_pred_flat)\n",
        "\n",
        "# Calculate the maximum value in the confusion matrix\n",
        "max_value = np.max(cm)\n",
        "\n",
        "# Create a color map for the heatmap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# Create a heatmap for the confusion matrix with color intensity\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, cbar=False, vmin=0, vmax=max_value)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.xticks(ticks=np.arange(len(category_labels)), labels=category_labels)\n",
        "plt.yticks(ticks=np.arange(len(category_labels)), labels=category_labels)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test_flat, final_pred_flat, target_names=category_labels)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "ksn80ABpoonn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Create the input for the meta-model by combining the predictions of the base models\n",
        "stacked_predictions = np.column_stack((prediction1, prediction2, prediction3))\n",
        "\n",
        "# Define the meta-model (can be any classifier of your choice)\n",
        "meta_model = RandomForestClassifier()\n",
        "\n",
        "# Train the meta-model on the stacked predictions\n",
        "meta_model.fit(stacked_predictions, test_y)\n",
        "\n",
        "# Make final predictions using the meta-model\n",
        "final_pred = meta_model.predict(stacked_predictions)\n",
        "\n",
        "# Evaluate the accuracy of the stacking ensemble\n",
        "accuracy = accuracy_score(test_y, final_pred)\n",
        "print(\"Accuracy of stacking ensemble:\", accuracy)\n"
      ],
      "metadata": {
        "id": "haj_BLM3oyQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming you have the true labels in y_test and the final predictions in final_pred\n",
        "\n",
        "# Flatten the true labels and final predictions\n",
        "y_test_flat = np.argmax(test_y, axis=1)\n",
        "final_pred_flat = np.argmax(final_pred, axis=1)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test_flat, final_pred_flat)\n",
        "\n",
        "# Create a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test_flat, final_pred_flat)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "DSGnh3t-o7O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDziWloZhvN8"
      },
      "outputs": [],
      "source": []
    }
  ]
}